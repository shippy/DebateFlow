"""Dataset card template generation for HuggingFace Hub."""

from __future__ import annotations

from collections import Counter
from pathlib import Path

from models import Debate


def _compute_stats(debates: list[Debate]) -> dict:
    """Compute summary statistics for the dataset card."""
    weakness_counts: Counter[str] = Counter()
    category_counts: Counter[str] = Counter()
    control_count = 0

    for d in debates:
        category_counts[d.metadata.category.value] += 1
        if d.metadata.is_control:
            control_count += 1
        else:
            assert d.metadata.constraint.type is not None
            weakness_counts[d.metadata.constraint.type.value] += 1

    return {
        "total": len(debates),
        "control": control_count,
        "constrained": len(debates) - control_count,
        "weakness_counts": dict(sorted(weakness_counts.items())),
        "category_counts": dict(sorted(category_counts.items())),
    }


def _size_category(n: int) -> str:
    if n < 1_000:
        return "n<1K"
    elif n < 10_000:
        return "1K<n<10K"
    else:
        return "10K<n<100K"


def generate_card(debates: list[Debate]) -> str:
    """Generate a HuggingFace dataset card README.md from debate data."""
    stats = _compute_stats(debates)

    weakness_lines = "\n".join(
        f"  - `{k}`: {v}" for k, v in stats["weakness_counts"].items()
    )
    category_lines = "\n".join(
        f"  - `{k}`: {v}" for k, v in stats["category_counts"].items()
    )

    return f"""---
language:
  - en
license: cc-by-4.0
task_categories:
  - text-classification
tags:
  - debate
  - argumentation
  - benchmark
  - llm-as-judge
  - synthetic
pretty_name: "DebateFlow"
size_categories:
  - {_size_category(stats["total"])}
---

# DebateFlow

A benchmark dataset of synthetic multi-turn debates for evaluating LLM debate judgment.

## Dataset Description

DebateFlow contains {stats["total"]} synthetic four-turn debates generated by LLMs arguing for and against stated resolutions. Each debate follows a structured format: Affirmative opening, Negative response, Affirmative rebuttal, Negative closing.

A subset of debates have **controlled asymmetries** — one side is constrained to exhibit a specific weakness (weak evidence, argument dropping, logical gaps, or burden-of-proof failure). This enables fine-grained evaluation of whether judge models can detect specific argumentation failures.

## Dataset Structure

Each instance contains:

- `metadata.debate_id` — unique 8-character identifier
- `metadata.resolution` — the debate topic
- `metadata.category` — `policy`, `values`, or `empirical`
- `metadata.aff_model` / `metadata.neg_model` — LLM configuration for each side
- `metadata.constraint.type` — injected weakness type (null for controls)
- `metadata.constraint.target_side` — which side was constrained (null for controls)
- `metadata.is_control` — whether this is a control debate (no injected weakness)
- `turns` — list of 4 turns, each with `speaker`, `role`, and `text`

## Dataset Statistics

- **Total debates**: {stats["total"]}
- **Control debates**: {stats["control"]}
- **Constrained debates**: {stats["constrained"]}

**By weakness type:**
{weakness_lines}

**By category:**
{category_lines}

## Dataset Creation

Debates are generated using an LLM-vs-LLM protocol where each side argues a resolution across four turns. For constrained debates, the weaker side receives hidden instructions to exhibit a specific argumentation weakness. The weakness is calibrated to be detectable by an attentive judge but not cartoonishly obvious.

### Constraint Types

- **weak_evidence**: Relies on anecdotes and vague authorities; structure is coherent but evidence is thin
- **argument_dropping**: Ignores 1-2 key opponent arguments without acknowledgment
- **logical_gaps**: Includes subtle fallacies (hasty generalization, false dichotomy, non-sequitur)
- **burden_of_proof**: Asserts without support and shifts burden to opponent

## Considerations for Using the Data

- All debates are synthetically generated and may not match the strategic depth of experienced human debaters
- LLM-generated text is stylistically more homogeneous than real competitive debate
- English-only
- Constraint effectiveness varies — some injected weaknesses may be too subtle or too obvious

## Additional Information

- **License**: CC-BY-4.0
- **Author**: Simon Podhajsky
- **Generator version**: {debates[0].metadata.generator_version if debates else "0.1.0"}
"""


def load_debates_from_jsonl(jsonl_path: Path) -> list[Debate]:
    """Load debates from a JSONL file."""
    debates = []
    with jsonl_path.open() as f:
        for line in f:
            line = line.strip()
            if line:
                debates.append(Debate.model_validate_json(line))
    return debates
